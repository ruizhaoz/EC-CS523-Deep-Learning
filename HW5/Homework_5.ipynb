{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruizhaoz/EC-CS523-Deep-Learning/blob/master/HW5/Homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l2K7f1ZwG5Z"
      },
      "source": [
        "# HW5: Transformer\n",
        "\n",
        "\n",
        "Designed by Ruizhao Zhu with help of Brian Kulis, Ashok Cutkosky.\n",
        "\n",
        "This assignment will introduce you to \n",
        "\n",
        "1. Understanding the structure of transformer. \n",
        "\n",
        "2. Building a GPT model step by step\n",
        "\n",
        "You can run this assignment on Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tMPpAqRpR2N"
      },
      "source": [
        "## Q1 Sequence to Sequence Modelling with nn.Transformer and Torch Text (20 points)\n",
        "\n",
        "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"360em\">\n",
        "\n",
        "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
        "\n",
        "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (another module recently\n",
        "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is now highly\n",
        "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
        "in this tutorial) can be easily adapted/composed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkeaGn8INY9k"
      },
      "source": [
        "### Q1.1 Define the model \n",
        "In this question, we train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked. To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ruizhaoz/ruizhaoz.github.io/master/encoder.png\n",
        "\" width=\"em\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHI3LBIcgGVO"
      },
      "source": [
        "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai9dTxjUNS5-"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    '''\n",
        "    This is a transformer encoder model, the input arguments are as follows:\n",
        "    args:\n",
        "    ntoken:  dimension of tokens\n",
        "    ninp: dimension of input embeddings\n",
        "    nhid: dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
        "    nlayers: number of TransformerEncoderLayer layers\n",
        "    nhead: the number of heads in the multiheadattention model\n",
        "    '''\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout) # PositionalEncoding will be implemented in next section.\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        '''\n",
        "        You can use torch.triu and masked_fill to get an upper triangle mask. \n",
        "        The upper right entries are -inf, down left entries including the diagonal are 0.\n",
        "        '''\n",
        "        mask = None\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        '''\n",
        "        Fill the forward function accoreding to the diagram above.\n",
        "        In the embedding layers, we multiply those weights by square root of \n",
        "        self.ninp.\n",
        "        '''\n",
        "        output = None\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-BxUnw6MZf5"
      },
      "source": [
        "### Q1.2 Positional Encoding\n",
        "#### Q1.2.1 Fill the code block\n",
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0pUKNMpQ84"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7mD6P7eCaKs"
      },
      "source": [
        "#### Q1.2.2 Why do we need this positional encoding in the transformer architectrue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNI5lKuvCmHu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i82IJ2chNstR"
      },
      "source": [
        "### Q1.3 Running the model\n",
        "\n",
        "#### Q1.3.1 Run the code to get desired performance.\n",
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv4yr-K3OHSs",
        "outputId": "62cda8b5-2289-4a5b-e08e-6643475a2e67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:01<00:00, 3.08MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjC_fNXWOcIJ"
      },
      "source": [
        "``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](../_static/img/transformer_input_target.png)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxe4vOD8Oh7S"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ss9NoUZY0Rz"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if data.size(0) != bptt:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5niRh2AI6qeN"
      },
      "source": [
        "Running the code block below, it will take about half an hour on colab. You will get around 220 ppl on training at the end of epoch 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CB2g1K5ZDBf"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 1 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgNfblVjucsp"
      },
      "source": [
        "#### 1.3.2 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpFEU0VqZMDl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ-XugehY_0y"
      },
      "source": [
        "##Q2 Transformer Block for GPT (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SucjfHGh_q5v"
      },
      "source": [
        "### Q 2.1 Multi-head self-attention\n",
        "#### Q 2.1.1 The first part is multi-head self-attention. In this layer, you will need to:\n",
        "- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value. The input and output size of linear projection are both `n_embd`\n",
        "- Apply attention, scaling the logits by $\\frac{1}{\\sqrt{d_{qkv}}}$.\n",
        "- Ensure proper masking, such that padding tokens are never attended to.\n",
        "- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/332139525/figure/fig3/AS:743081083158528@1554175744311/a-The-Transformer-model-architecture-b-left-Scaled-Dot-Product-Attention.ppm\" width=\"360em\">\n",
        "\n",
        "You should include two types of dropout in your code (with probability set by the  `dropout` argument):\n",
        "- Dropout should be applied to the output of the attention layer (just prior to the residual connection, denoted by \"Add & Norm\" in the first figure)\n",
        "- Dropout should *also* be applied after the final projection.\n",
        "Notes:\n",
        "- Query, key, and value vectors should have shape `[batch_size, n_heads, sequence_len, d_qkv]`\n",
        "- Apply a mask to the scaled dot product of Q and K, before the Softmax function. Let the entry to be a small enough number where the entry of the causal mask is zero. You can use `torch.tril` or `torch.triu` to create a mask, usually we define the mask as a lower triangular matrix. Lower left (incude the diagonal) entries are ones, rest of entries are zeros.\n",
        "Then apply `tensor.masked_fill()` to the output of the scaled dot product of Q and K (It is also the input of softmax). Where the mask is zero, set the input to softmax to a negative number with very large magnitude.\n",
        "- Attention logits and probabilities should have shape `[batch_size, n_heads, sequence_len, sequence_len]`\n",
        "- Vaswani et al. define the output of the attention layer as concatenating the various heads and then multiplying by a matrix $W^O$. It's also possible to implement this is a sum without ever calling `torch.cat`: note that $\\text{Concat}(head_1, \\ldots, head_h)W^O = head_1 W^O_1 + \\ldots + head_h W^O_h$ where $W^O = \\begin{bmatrix} W^O_1\\\\ \\vdots\\\\ W^O_h\\end{bmatrix}$. You may define the `self.proj` this way.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-81DlPewvDRR"
      },
      "source": [
        "import math\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    You can also use torch.nn.MultiheadAttention to validate your implementation\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        #Define key, query, value projections for all heads\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        self.key = None\n",
        "        self.query = None\n",
        "        self.value = None\n",
        "        # Dropout layers\n",
        "        self.attn_drop = None\n",
        "        self.resid_drop = None\n",
        "        # output projection\n",
        "        self.proj = None\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.mask = None\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size() # B = Batch\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        output = None\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9rHSW_iV8Zl"
      },
      "source": [
        "#### Q 2.1.2 Why do we need to divide a scale of the dot product of Q and K?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g64isWe2WXYW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKJV23RLAKT3"
      },
      "source": [
        "### Q2.2 Transformer\n",
        "We will implement the transformer block, which is the blue box in the figure. You can use `nn.LayerNorm` layer to apply layer norm. We defined the feed forward layer as `self.mlp`.\n",
        "\n",
        "Notice that where to use the layer norm is a design choice, you can change to see how it affect the final results in the application of Question 3.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ruizhaoz/ruizhaoz.github.io/master/Screen%20Shot%202020-11-05%20at%2011.37.09%20AM.png\" width=\"240em\">\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_L0P2zHAOVt"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" an Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"YOUR CODE HERE?\"\"\" \n",
        "        \n",
        "        return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygPhx7D_Bsga"
      },
      "source": [
        "## Q3 GPT on Addition (40 points)\n",
        "In this question, we will train an GPT transformer to do addition. We first need to get the dataset and encode addition equation to a vocabulary by integers since we want to use GPT dealing with sequences of integers, and completing them according to patterns in the data. \n",
        "\n",
        "  The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "  encoding will simply be the n-digit first number, n-digit second number, and (n+1)-digit result, all simply concatenated together. Because each addition problem is so structured, there is no need to bother the model with encoding +, =, or other tokens. Each possible sequence has the same length, and simply contains the raw digits of the addition problem. As a few examples, the 2-digit problems:\n",
        "- 85 + 50 = 135 becomes the sequence `[8, 5, 5, 0, 1, 3, 5]`\n",
        "- 6 + 39 = 45 becomes the sequence `[0, 6, 3, 9, 0, 4, 5]`\n",
        "\n",
        "We will also only train GPT on the final (n+1)-digits because the first two n-digits are always assumed to be given. So when we give GPT an exam later, we will e.g. feed it the sequence `[0, 6, 3, 9]`, which encodes that we'd like to add 6 + 39, and hope that the model completes the integer sequence with `[0, 4, 5]` in 3 sequential steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGuOjDBGX3H"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwKTM2RgGQ2n"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Define the addition dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "        \n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100 \n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lru7Mb4fGV0h",
        "outputId": "46d9dcef-2de6-48bc-d0c3-f1a1491bf936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a dataset for e.g. 2-digit addition\n",
        "ndigit = 2\n",
        "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
        "test_dataset = AdditionDataset(ndigit=ndigit, split='test')\n",
        "train_dataset[0] # sample a training instance just to see what one raw example looks like"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uZgtgQlJNEN"
      },
      "source": [
        "### Q3.2 Define the GPT model \n",
        "Now, we start constructing the GPT model. As is shown in the figure, there are 12 transformer blocks concatenated together. In our model, we use `n_layer` to represent the number of blocks. In this question, you need to do the following:\n",
        "\n",
        "- Define the `n_layer` transformer blocks `self.blocks`\n",
        "- Fill out the forward function. Note that the positional embedding is not hard coded as the original transformer, it is learned during training.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ruizhaoz/ruizhaoz.github.io/master/GPT1.png\" width=\"240em\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCozm4EEByR7"
      },
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a squence size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_layer, embd_pdrop=0.1, attn_pdrop=0.1,resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
        "        self.drop = nn.Dropout(embd_pdrop)\n",
        "        # transformer\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        self.blocks = None\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        You don't need to change this function. This is setting specific parameters for optimization.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        b, t = x.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        logits = None\n",
        "        loss = None\n",
        "\n",
        "        return logits, loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWEYZY9DF4y7"
      },
      "source": [
        "###Q3.3 Training the model\n",
        "\n",
        "##### You will train the GPT model. Fill out the code of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca4alq_1GOM5"
      },
      "source": [
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA7CPuribP_h"
      },
      "source": [
        "Setting some parameters for training. Initialize the GPT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsl9tjHrGOvt"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "# initialize a baby GPT model\n",
        "model = GPT(vocab_size = train_dataset.vocab_size, n_embd=128, n_head=4, block_size =  train_dataset.block_size, n_layer=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sgav_e5boOD"
      },
      "source": [
        "You need to fill out training process. \n",
        "- Forward the model with current batch `x`, `y`;\n",
        "- Zero the grad before update;\n",
        "- Backward the loss and update the model parameter;\n",
        "- You might want to use `torch.nn.utils.clip_grad_norm_`. The parameter max_norm is `config.grad_norm_clip`;\n",
        "- You will run this about 10 min getting a loss around 0.005."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIhTJaPsM7LD"
      },
      "source": [
        "config = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "#trainer = Trainer(model, train_dataset, test_dataset, config)\n",
        "#trainer.train()\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.cuda.current_device()\n",
        "  model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "\n",
        "optimizer = model.configure_optimizers(config)\n",
        "tokens = 0\n",
        "for epoch in range(config.max_epochs):\n",
        "    model.train()\n",
        "    data = train_dataset \n",
        "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                        batch_size=config.batch_size,\n",
        "                        num_workers=config.num_workers)\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total=len(loader)) \n",
        "    for iter, (x, y) in pbar:\n",
        "        # place data on the correct device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # forward the model\n",
        "        \"\"\" CODE HERE \"\"\"\n",
        "        loss = None\n",
        "        \"\"\" CODE HERE END \"\"\"\n",
        "        losses.append(loss.item())\n",
        "        # decay the learning rate based on our progress\n",
        "        if config.lr_decay:\n",
        "            tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "            if tokens < config.warmup_tokens:\n",
        "                # linear warmup\n",
        "                lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
        "            else:\n",
        "                # cosine learning rate decay\n",
        "                progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "            lr = config.learning_rate * lr_mult\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            lr = config.learning_rate\n",
        "        # report progress\n",
        "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0gIIlMJiSFC"
      },
      "source": [
        "Now you can run the following code to test the training data sne testing data. You should reach more than 95% correctness on both train and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gARh2KE3K5A"
      },
      "source": [
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. \n",
        "    \"\"\"\n",
        "    block_size = train_dataset.block_size\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    return x\n",
        "def Addition_GPT(dataset, batch_size=32, max_batches=-1):\n",
        "    \n",
        "    results = []\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        d1d2 = x[:, :ndigit*2]\n",
        "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[:, -(ndigit+1):]\n",
        "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(device)\n",
        "        # decode the integers from individual digits\n",
        "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
        "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
        "        d3i_pred = (d3 * factors).sum(1)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            judge = 'CORRECT' if correct[i] else 'WRONG'\n",
        "            if not correct[i]:\n",
        "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
        "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
        "        \n",
        "        if max_batches >= 0 and b+1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbHtf3tqjJAa"
      },
      "source": [
        "Addition_GPT(train_dataset, batch_size=1024, max_batches=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_sljQGijJ0n"
      },
      "source": [
        "Addition_GPT(test_dataset, batch_size=1024, max_batches=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}